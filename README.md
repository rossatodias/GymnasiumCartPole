# Projeto CartPole - Controle Manual e Aprendizado por Refor√ßo

Este reposit√≥rio cont√©m a implementa√ß√£o do **Projeto 1**, que inclui:

1. **Controle Manual (Bang-Bang):** Um operador humano tenta equilibrar o p√™ndulo usando as teclas do teclado.
2. **Aprendizado por Refor√ßo:** Compara√ß√£o entre dois algoritmos cl√°ssicos ‚Äî **Q-Learning Tabular** e **DQN (Deep Q-Network)**.

Ambos utilizam o ambiente **CartPole-v1** da biblioteca Gymnasium.

---

## üìã Pr√©-requisitos

Para executar este c√≥digo, voc√™ precisar√° do **Python 3.7+** instalado.

As depend√™ncias necess√°rias s√£o:
- `gymnasium`: Para o ambiente de simula√ß√£o e f√≠sica.
- `pygame`: Para capturar eventos do teclado e gerenciar a janela de renderiza√ß√£o.
- `numpy`: Para c√°lculos matem√°ticos auxiliares.
- `matplotlib`: Para gera√ß√£o de gr√°ficos comparativos.
- `torch` (PyTorch): Para implementa√ß√£o da rede neural do DQN.

---

## üöÄ Instala√ß√£o

### 1Ô∏è‚É£ Clonar o reposit√≥rio
```bash
git clone https://github.com/rossatodias/GymnasiumCartPole.git
cd GymnasiumCartPole
```

### 2Ô∏è‚É£ Criar e ativar um ambiente virtual (recomendado)
```bash
python3 -m venv venv        # Linux/macOS
source venv/bin/activate    # Linux/macOS

python -m venv venv         # Windows
venv\Scripts\activate       # Windows
```

### 3Ô∏è‚É£ Instalar as depend√™ncias dentro do venv
```bash
pip install -r requirements.txt
```

---

## üéÆ Modo 1: Controle Manual (`human.py`)

Este modo permite que um operador humano tente equilibrar o p√™ndulo utilizando as teclas do teclado. Para tornar a tarefa vi√°vel e did√°tica, considerando o tempo de rea√ß√£o humano, foram aplicadas modifica√ß√µes na f√≠sica e na velocidade de renderiza√ß√£o.

### Como Executar

```bash
python3 human.py        # Linux/macOS
python human.py         # Windows
```

### üïπÔ∏è Controles

A mec√¢nica de controle √© do tipo **Bang-Bang**, onde uma for√ßa fixa √© aplicada em uma dire√ß√£o.

- **Seta Esquerda (‚Üê):** Aplica for√ßa m√°xima para a esquerda.
- **Seta Direita (‚Üí):** Aplica for√ßa m√°xima para a direita.
- **Tecla Q:** Encerra a simula√ß√£o e exibe o resumo das recompensas.

> **Nota:** Se nenhuma tecla for pressionada, o ambiente continuar√° aplicando a √∫ltima a√ß√£o ou o padr√£o, por isso s√£o necess√°rias corre√ß√µes constantes.

### ‚öôÔ∏è Configura√ß√µes

Para facilitar o controle por humanos, foram aplicados os seguintes ajustes:

| Par√¢metro | Valor | Motivo |
|-----------|-------|--------|
| **FPS** | 10 (Slow Motion) | Compensa o delay de rea√ß√£o visual-motor humano |
| **Limite de √Çngulo** | 45¬∞ (padr√£o: 12¬∞) | Oferece janela de recupera√ß√£o maior |

---

## ü§ñ Modo 2: Aprendizado por Refor√ßo (`rlearning.py`)

Este modo treina e compara dois algoritmos de aprendizado por refor√ßo:

### Algoritmos Implementados

#### Q-Learning Tabular
- Discretiza o espa√ßo de estados cont√≠nuo em bins
- Armazena Q-values em uma tabela multidimensional
- Ideal para espa√ßos de estados pequenos/m√©dios
- **Tamanho da tabela:** ~16.562 valores

#### DQN (Deep Q-Network)
- Utiliza rede neural para aproximar a fun√ß√£o Q
- **Experience Replay:** Armazena transi√ß√µes para treinamento em batch
- **Target Network:** Rede separada para estabilidade do treinamento
- Ideal para espa√ßos de estados cont√≠nuos/grandes

### Como Executar

```bash
python3 rlearning.py        # Linux/macOS
python rlearning.py         # Windows
```

O script ir√°:
1. Treinar o agente Q-Learning por 650 epis√≥dios
2. Treinar o agente DQN por 650 epis√≥dios
3. Gerar um gr√°fico comparativo (`comparacao_rl.png`)
4. Demonstrar visualmente ambos os agentes treinados

### ‚öôÔ∏è Hiperpar√¢metros

Os principais hiperpar√¢metros podem ser ajustados no in√≠cio do arquivo:

| Par√¢metro | Valor Padr√£o | Descri√ß√£o |
|-----------|--------------|-----------|
| `EPISODES` | 650 | Total de epis√≥dios de treinamento |
| `MAX_STEPS` | 500 | Limite de passos por epis√≥dio |
| `GAMMA` | 0.99 | Fator de desconto |
| `EPSILON_START` | 1.0 | Explora√ß√£o inicial (100% aleat√≥rio) |
| `EPSILON_END` | 0.01 | Explora√ß√£o final (1% aleat√≥rio) |
| `EPSILON_DECAY` | 0.995 | Taxa de decaimento da explora√ß√£o |
| `LEARNING_RATE_Q` | 0.1 | Taxa de aprendizado (Q-Learning) |
| `LEARNING_RATE_DQN` | 0.001 | Taxa de aprendizado (DQN) |
| `BATCH_SIZE` | 64 | Tamanho do batch (DQN) |

### üìà Sa√≠da

- **Terminal:** Progresso do treinamento a cada 50 epis√≥dios (m√©dia, m√°ximo, epsilon)
- **Gr√°fico:** Arquivo `comparacao_rl.png` com curvas de aprendizado suavizadas
- **Demonstra√ß√£o:** Execu√ß√£o visual de ambos os agentes ap√≥s o treinamento

---

## üìä Estrutura do Projeto

```
GymnasiumCartPole/
‚îú‚îÄ‚îÄ human.py           # Controle manual (bang-bang)
‚îú‚îÄ‚îÄ rlearning.py       # Aprendizado por refor√ßo (Q-Learning + DQN)
‚îú‚îÄ‚îÄ requirements.txt   # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ comparacao_rl.png  # Gr√°fico gerado (ap√≥s execu√ß√£o)
‚îî‚îÄ‚îÄ README.md          # Este arquivo
```

---

## üîß Solu√ß√£o de Problemas

### VS Code
O VS Code deve perguntar: *"We noticed a new environment... do you want to select it?"* ‚Üí Clique em **Yes**. Se n√£o perguntar, fa√ßa o passo abaixo e selecione o `venv` que voc√™ acabou de criar.

### Selecione o Interpretador Correto no VS Code
Se voc√™ n√£o selecionou a op√ß√£o anterior, provavelmente o VS Code estar√° olhando para um Python diferente do que voc√™ usou para instalar.

1. Abra seu arquivo `.py` no VS Code.
2. Pressione **`Ctrl + Shift + P`** (ou `Cmd + Shift + P` no Mac).
3. Digite e selecione: **`Python: Select Interpreter`**.
4. Vai aparecer uma lista.
   - Se voc√™ criou um ambiente virtual (pasta `.venv` ou `venv`), selecione a op√ß√£o que tem `./venv/bin/python` ou similar.
   - Se n√£o criou, procure a vers√£o "Global" onde voc√™ rodou o `pip install`.
   - *Dica: Geralmente a op√ß√£o "Recommended" √© a correta.*

---
